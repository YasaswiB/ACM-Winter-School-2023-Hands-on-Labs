{"cells":[{"cell_type":"markdown","metadata":{"id":"LmLFqOP5bMOj"},"source":["**MOUNTING DATASET FROM GOOGLE DRIVE TO GOOGLE COLAB**\n","\n","### FAQs for the Provided Code:\n","\n","**Q1: What does the first line of code do?**\n","*The first line of code imports the Google Drive module from the `google.colab` library.*\n","\n","**Q2: What is the purpose of the second line of code?**\n","*The second line of code mounts the Google Drive to the Colab environment, allowing access to files stored in Google Drive.*\n","\n","**Q3: Why is there a folder path specified in the third line of code?**\n","*The third line of code assigns a path to the variable `folder_path`, indicating the location of the folder in Google Drive that contains the CSV file.*\n","\n","**Q4: What is the significance of the fourth line of code?**\n","*The fourth line of code concatenates the `folder_path` with the specific CSV file name (`PHEME_preprocessed.csv`) to create the full path (`csv_file_path`) to the CSV file.*\n","\n","**Q5: Why is it necessary to mount Google Drive in Colab?**\n","*Mounting Google Drive in Colab is necessary to access and interact with files stored in Google Drive from within the Colab environment. This allows for seamless integration of data stored in Google Drive with Colab notebooks.*\n","\n","**Q6: Can I use this code with my own Google Drive folder and CSV file?**\n","*Yes, you can. Modify the `folder_path` variable to point to your own folder in Google Drive, and update the CSV file name in the `csv_file_path` variable accordingly.*\n","\n","**Q7: What should I do if I encounter issues with Google Drive mounting?**\n","*Ensure that you follow the authentication steps prompted by Colab when running the mount code. Also, check your Google Drive permissions and make sure that the specified folder and file paths are correct.*\n","\n","**Q8: Can I load other types of files using a similar approach?**\n","*Yes, you can modify the code to load various file types. The key is to adjust the file path accordingly, depending on the file type you are working with.*\n","\n","**Q9: Is there an alternative way to load files into Colab without using Google Drive?**\n","*Yes, you can upload files directly to Colab using the upload button in the Colab interface. Alternatively, you can use methods like `wget` or `curl` to fetch files from the internet directly into your Colab environment.*"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZh4gtYdWW1D","executionInfo":{"status":"ok","timestamp":1703498556228,"user_tz":-330,"elapsed":4154,"user":{"displayName":"Yasaswi B","userId":"00339005883420863081"}},"outputId":"cbe82b80-5134-465e-e00f-1f45707854b2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":480,"status":"ok","timestamp":1703498595620,"user":{"displayName":"Yasaswi B","userId":"00339005883420863081"},"user_tz":-330},"id":"I97NmTQlXzyo"},"outputs":[],"source":["\n","# Path to the folder containing your CSV file\n","#folder_path = '/content/drive/MyDrive/fake-news-hands-on/'\n","\n","# Path to the CSV file\n","#csv_file_path = folder_path + 'PHEME_preprocessed.csv'\n","csv_file_path = '/content/drive/MyDrive/fake-news-hands-on-20231225T095255Z-001/fake-news-hands-on/PHEME_preprocessed.csv'"]},{"cell_type":"markdown","metadata":{"id":"GcPXxhYxbb5s"},"source":["**READING DATASET INTO A DATAFRAME**\n","\n","### FAQs for the below Code:\n","\n","**Q1: What does the first line of code (`import pandas as pd`) do?**\n","*The first line imports the Pandas library and assigns it the alias 'pd'. This alias is commonly used to reference Pandas functions and objects in a more concise manner.*\n","\n","**Q2: What is the purpose of the second line (`df = pd.read_csv(csv_file_path)`) in the code?**\n","*The second line reads the CSV file specified by the `csv_file_path` variable into a Pandas DataFrame (`df`). This allows for easy manipulation and analysis of the data in a tabular format.*\n","\n","**Q3: Why is it necessary to use the `pd.read_csv` function?**\n","*The `pd.read_csv` function is specifically designed to read data from CSV files and create a DataFrame in Pandas. It automatically handles the parsing of CSV data into a structured tabular format.*\n","\n","**Q4: What does the third line (`print(df.head())`) do?**\n","*The third line prints the first few rows of the DataFrame (`df`) using the `head()` method. This is a common practice to quickly inspect the structure and content of the loaded data.*\n","\n","**Q5: How can I modify the code to display a different number of rows in the printed output?**\n","*You can customize the number of rows displayed by specifying a parameter inside the `head()` method. For example, `print(df.head(10))` will display the first 10 rows.*\n","\n","**Q6: What should I do if the CSV file contains missing values or special characters?**\n","*You can use Pandas functions such as `dropna()` or `fillna()` to handle missing values, and `replace()` to manage special characters. It depends on the specific data cleaning requirements.*\n","\n","**Q7: Can I perform data analysis or visualization on this DataFrame after loading it?**\n","*Yes, Pandas provides a wide range of functions for data analysis, manipulation, and visualization. You can explore and analyze the data using various Pandas functions or integrate with other libraries like Matplotlib or Seaborn for visualization.*\n","\n","**Q8: How can I access specific columns or rows in the DataFrame?**\n","*You can use column names to access specific columns (`df['column_name']`) and functions like `loc` or `iloc` to access specific rows or slices of the DataFrame.*\n","\n","**Q9: What happens if the CSV file is not formatted correctly or has errors?**\n","*If the CSV file is not formatted correctly or contains errors, the `pd.read_csv` function may raise an error. It's essential to ensure that the CSV file follows the correct format and fix any issues before reading it into a DataFrame.*\n","\n","**Q10: Can I save modifications made to the DataFrame back to a CSV file?**\n","*Yes, Pandas provides the `to_csv` method to save a DataFrame to a CSV file. You can use `df.to_csv('new_file.csv')` to save the DataFrame to a new CSV file.*\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":747,"status":"ok","timestamp":1703498600930,"user":{"displayName":"Yasaswi B","userId":"00339005883420863081"},"user_tz":-330},"id":"5qC3stG9bBfo","outputId":"3bb16139-8117-4d3d-ef70-bb6e15490125"},"outputs":[{"output_type":"stream","name":"stdout","text":["   Unnamed: 0            tweet_id         event  \\\n","0           0  553477352542707712  charliehebdo   \n","1           1  553589285342175232  charliehebdo   \n","2           2  552810804186460161  charliehebdo   \n","3           3  553535971959275520  charliehebdo   \n","4           4  553579224402235393  charliehebdo   \n","\n","                                                text category  \n","0  Breaking Police suspect gunmen have taken one ...   rumour  \n","1  BREAKING NEWS Killed Charlie Hebdo suspects ca...   rumour  \n","2  #CharlieHebdo famous cartoonists Charb and Cab...   rumour  \n","3  #BREAKING 1 injured in #Paris Kosher supermark...   rumour  \n","4  Hostage taker at Paris store demands release o...   rumour  \n"]}],"source":["import pandas as pd\n","\n","# Read the CSV file\n","df = pd.read_csv(csv_file_path)\n","\n","# Display the dataframe\n","print(df.head())\n"]},{"cell_type":"markdown","metadata":{"id":"o1klCkVwdhae"},"source":["**STATS OF PHEME DATASET**\n","\n","### FAQs for the Below Code:\n","\n","**Q1: What is the purpose of the first line (`df = df.drop(columns=['Unnamed: 0'], errors='ignore')`) in the code?**\n","*This line drops the column named 'Unnamed: 0' from the DataFrame `df`. The `errors='ignore'` parameter is used to ignore errors if the specified column does not exist.*\n","\n","**Q2: Why would we want to drop the 'Unnamed: 0' column?**\n","*The 'Unnamed: 0' column is often an index column added during data export or processing. It may not contain meaningful information, and dropping it helps to clean up the DataFrame.*\n","\n","**Q3: How can I drop multiple columns from the DataFrame if needed?**\n","*You can modify the code to drop multiple columns by passing a list of column names to the `drop` method, like this: `df = df.drop(columns=['Column1', 'Column2'])`.*\n","\n","**Q4: What does the second block of code (`news_counts = df['event'].value_counts()`) do?**\n","*This code calculates and displays the counts of unique values in the 'event' column using the `value_counts()` method. It provides insights into the distribution of different news events in the dataset.*\n","\n","**Q5: How can I adapt this code to analyze a different column in the DataFrame?**\n","*Simply replace 'event' with the name of the column you want to analyze. For example, to analyze the 'column_name', use `df['column_name'].value_counts()`.*\n","\n","**Q6: What information does the output of `print(\"Different news event values and their counts:\")` provide?**\n","*This line is a print statement that indicates the following lines will display the counts of different values in the 'event' column, providing an overview of the distribution of news events.*\n","\n","**Q7: How can I display the top N values instead of all values for the 'event' and 'category' columns?**\n","*You can use the `head(N)` method after the `value_counts()` to display the top N values. For example, `df['event'].value_counts().head(10)` will show the top 10 news events.*\n","\n","**Q8: What does the third block of code (`category_counts = df['category'].value_counts()`) do?**\n","*This code calculates and displays the counts of unique values in the 'category' column, similar to the previous block of code but for a different column.*\n","\n","**Q9: Can I visualize the counts of different categories using a plot or chart?**\n","*Yes, you can use libraries like Matplotlib or Seaborn to create visualizations based on the counts obtained. For example, a bar chart can be created to visualize the distribution of categories.*\n","\n","**Q10: What should I do if the 'category' or 'event' column has too many unique values to analyze easily?**\n","*Consider using visualizations like bar charts for a clearer overview, or focus on the top N values using the `head(N)` method to prioritize the most frequent categories or events.*"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":446,"status":"ok","timestamp":1703498604958,"user":{"displayName":"Yasaswi B","userId":"00339005883420863081"},"user_tz":-330},"id":"xD1WK-MddiaU","outputId":"0a89bebf-0722-4094-9c5d-7601d5c0371d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Different news event values and their counts:\n","charliehebdo         2079\n","sydneysiege          1221\n","ferguson             1143\n","ottawashooting        890\n","germanwings-crash     469\n","Name: event, dtype: int64\n","\n","Different 'category' values and their counts:\n","nonrumour    3830\n","rumour       1972\n","Name: category, dtype: int64\n"]}],"source":["# Drop the 'Unnamed' column\n","df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n","\n","# Show different 'news' values and their counts\n","news_counts = df['event'].value_counts()\n","print(\"Different news event values and their counts:\")\n","print(news_counts)\n","\n","# Show different 'category' values and their counts\n","category_counts = df['category'].value_counts()\n","print(\"\\nDifferent 'category' values and their counts:\")\n","print(category_counts)"]},{"cell_type":"markdown","metadata":{"id":"ZM9o7sD_o8tf"},"source":["**DOWNLOADING IMAGES FROM NET BY GIVING INPUT AS TEXT USING ICRAWLER**\n","\n","### FAQs for the iCrawler Code:\n","\n","**Q1: What does the first line (`!pip install icrawler`) do?**\n","*This line installs the iCrawler library using the pip package manager. It is a one-time installation to ensure that the necessary library is available in the Colab environment.*\n","\n","**Q2: What is the purpose of the second and third lines (`from icrawler.builtin import GoogleImageCrawler`)?**\n","*These lines import the `GoogleImageCrawler` class from the iCrawler library. This class is specifically designed for crawling and downloading images from Google Images.*\n","\n","**Q3: How does the `download_images` function work?**\n","*The `download_images` function takes a keyword, a save directory, and an optional number of images as parameters. It uses the `GoogleImageCrawler` to crawl Google Images for the specified keyword, filters the results based on size, license, and type, and then downloads the images to the specified directory.*\n","\n","**Q4: Can I customize the image download parameters, such as size and license?**\n","*Yes, the `filters` dictionary inside the function allows customization of parameters such as image size, license, and type. You can modify these parameters based on your requirements.*\n","\n","**Q5: What does the commented out example usage (`# download_images('cats', '/content/drive/MyDrive/fake-news-hands-on/cats_images', num_images=10)`) demonstrate?**\n","*This line demonstrates an example usage of the `download_images` function. It would download 10 images related to the keyword 'cats' and save them to the specified directory.*\n","\n","**Q6: How does the code snippet `df_part = df.iloc[40:50]` work?**\n","*This line creates a subset of the original DataFrame (`df`) by selecting rows 40 to 49 (inclusive) using integer-location based indexing (`iloc`). It is useful for testing the image download functionality on a smaller subset of data.*\n","\n","**Q7: What information does the loop (`for index, row in df_part.iterrows():`) iterate over?**\n","*The loop iterates over each row in the DataFrame subset (`df_part`). For each row, it extracts the 'text' content and 'tweet_id' columns, which are then used in the subsequent code to download images related to the text content.*\n","\n","**Q8: How is the image save path constructed in the loop (`image_save_path = f'/content/drive/MyDrive/fake-news-hands-on/images/{tweet_id}_image.jpg'`)?**\n","*The image save path is constructed by combining the specified directory (`'/content/drive/MyDrive/fake-news-hands-on/images/'`) with the tweet_id and '_image.jpg'. This ensures a unique path for each image based on its tweet_id.*\n","\n","**Q9: Can I download images for different keywords using this code?**\n","*Yes, you can modify the `download_images` function call inside the loop to use different keywords. The `text_content` variable is currently used as the keyword.*\n","\n","**Q10: What should I do if there are issues with image downloads?**\n","*Check the keyword used for the image search, and consider adjusting the filters inside the `download_images` function. Additionally, ensure that the save path and directory have the necessary permissions.*"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70881,"status":"ok","timestamp":1703500960194,"user":{"displayName":"Yasaswi B","userId":"00339005883420863081"},"user_tz":-330},"id":"cOHyiLCp2Czg","outputId":"c9836243-e892-4ecb-dcc2-2679d6a79f7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: icrawler in /usr/local/lib/python3.10/dist-packages (0.6.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from icrawler) (6.0.1)\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/191216_Stray_Kids_for_JYP_Ent_Audition_%282%29.png\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Stray_Kids_230109.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/190424_Stray_Kids_The_Fact_Music_Awards.png\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/SKZ.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/230311_Kang_Min-hee.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/SKZ_Telux_.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Singapore_Airlines_%289V-SKZ%29_Airbus_A380-841_at_Sydney_Airport_%283%29.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/9V-SKZ_05082018LHR_%2843901798811%29.jpg\n","ERROR:downloader:Response status code 404, file https://vi.m.wikipedia.org/wiki/T%25E1%25BA%25ADp_tin:Kingdom_mnet.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Oriental_magpie_1_skz.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Han_at_Golden_Disk_Awards_on_January_6%2C_2019.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/SKZ_Telux-Hafenstube_%C2%A9_Torsten_P%C3%B6tzsch_20190615_214933a.jpg\n","ERROR:downloader:Response status code 404, file https://commons.wikimedia.org/wiki/File:SKZ_Telux-Hafenstube_%25C2%25A9_Torsten_P%25C3%25B6tzsch_20190615_214933a.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Upravlenie_SKZhD.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Stray_Kids_Changbin.jpg\n","ERROR:downloader:Response status code 404, file https://vi.m.wikipedia.org/wiki/T%25E1%25BA%25ADp_tin:Stray_Kids_Changbin.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Avensis_%28Romsey%29_coach_%28SKZ_5682%29%2C_15_January_2011_%282%29.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Avensis_%28Romsey%29_coach_%28SKZ_5682%29%2C_15_January_2011_%281%29.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Toda_kyoutei_1.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Lee_Know_modeling_for_BEAUTY%2B_on_January_26%2C_2022_05.jpg\n","ERROR:downloader:Response status code 404, file https://commons.wikimedia.org/wiki/File:Lee_Know_modeling_for_BEAUTY%252B_on_January_26,_2022_05.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Meril_Life_Sciences_01.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Marek_Kossakowski%2C_May_Day_celebration%2C_Grzybowski_square%2C_Warsaw%2C_May_1%2C_2016.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Duple_320_%2817850568172%29.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Lee_Know_modeling_for_BEAUTY%2B_on_January_26%2C_2022_04.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Phekoo_with_Exile_Backstage_at_the_55th_Japan_Record_Award_Show.jpg\n","ERROR:downloader:Response status code 404, file https://commons.wikimedia.org/wiki/File:Lee_Know_modeling_for_BEAUTY%252B_on_January_26,_2022_04.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Meril_Life_Sciences_02.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Hubava_Bogdanka.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Klemens_Gerner_gr%C3%B3b.jpg\n","ERROR:downloader:Response status code 404, file https://commons.wikimedia.org/wiki/File:Klemens_Gerner_gr%25C3%25B3b.jpg\n","ERROR:downloader:Response status code 404, file https://en.m.wikipedia.org/wiki/File:%25D0%2592%25D1%258B%25D0%25B1%25D0%25BE%25D1%2580%25D0%25B3%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25B2%25D0%25BE%25D0%25B5%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B3%25D0%25BE%25D1%2581%25D0%25BF%25D0%25B8%25D1%2582%25D0%25B0%25D0%25BB%25D1%258C.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/%D0%92%D1%8B%D0%B1%D0%BE%D1%80%D0%B3%D1%81%D0%BA%D0%B8%D0%B9_%D0%B2%D0%BE%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B3%D0%BE%D1%81%D0%BF%D0%B8%D1%82%D0%B0%D0%BB%D1%8C.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/CLC_Seungyeon%2C_March_2016.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Toda_kyoutei_2.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Lake_near_the_village_of_Horbachevo-Mykhailivka_%E2%80%94_%D0%9E%D0%B7%D0%B5%D1%80%D0%BE_%D0%B1%D1%96%D0%BB%D1%8F_%D1%81%D0%B5%D0%BB%D0%B0_%D0%93%D0%BE%D1%80%D0%B1%D0%B0%D1%87%D0%B5%D0%B2%D0%BE-%D0%9C%D0%B8%D1%85%D0%B0%D0%B9%D0%BB%D1%96%D0%B2%D0%BA%D0%B0_01.jpg\n","ERROR:downloader:Response status code 404, file https://commons.wikimedia.org/wiki/File:Lake_near_the_village_of_Horbachevo-Mykhailivka_%25E2%2580%2594_%25D0%259E%25D0%25B7%25D0%25B5%25D1%2580%25D0%25BE_%25D0%25B1%25D1%2596%25D0%25BB%25D1%258F_%25D1%2581%25D0%25B5%25D0%25BB%25D0%25B0_%25D0%2593%25D0%25BE%25D1%2580%25D0%25B1%25D0%25B0%25D1%2587%25D0%25B5%25D0%25B2%25D0%25BE-%25D0%259C%25D0%25B8%25D1%2585%25D0%25B0%25D0%25B9%25D0%25BB%25D1%2596%25D0%25B2%25D0%25BA%25D0%25B0_01.jpg\n","ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Gift_Ice_Story_2023_-_Tokyo_Dome_outdoor_view.jpg\n"]},{"output_type":"stream","name":"stdout","text":["text_content: Shots fired and hostages taken in a building east of Paris as police hunt #CharlieHebdo attackers \n","text_content: #ChalieHebdo terrorists are held up in a warehouse with hostages #JeSuisCharlie \n","text_content: French media reports two suspects of #CharlieHebdo attack are killed and supermarket hostage taker also killed \n","text_content: Hostages have reportedly been taken as police pursue gunmen responsible for the #ParisAttack \n","text_content: Cartoonist Stephane Charbonnier was critically injured in the attack LeFigaro #CharlieHebdo \n","text_content: Convoy of police cars rushing toward town northeast of Paris amid reports of hostages taken broadcast \n","text_content: Police Four of France's most celebrated cartoonists killed Charb Cabu Wolinski amp Tignous #CharlieHebdo\n","text_content: #CHARLIEHEBDO SHOOTING Gunmen shouted 'we have avenged the prophet during attack reports \n","text_content: R I P Ahmed Merabet a French #Muslim Cop first victim of #CharlieHebdo attack \n","text_content: Reports that the Jewish community in #France going in to lockdown Shops synagogues schools etc #ParisShooting #CharlieHebdo\n"]}],"source":["!pip install icrawler\n","\n","from icrawler.builtin import GoogleImageCrawler\n","\n","# Function to download images using iCrawler\n","def download_images(keyword, save_directory, num_images=10):\n","    google_crawler = GoogleImageCrawler(\n","        downloader_threads=4,  # You can adjust the number of downloader threads\n","        storage={'root_dir': save_directory}\n","    )\n","\n","    filters = dict(\n","        size='medium',  # You can adjust the size of the images ('small', 'medium', 'large', 'icon', 'ipad', 'iphone')\n","        license='commercial,modify',\n","        type='photo'\n","    )\n","\n","    google_crawler.crawl(keyword=keyword, max_num=num_images, filters=filters)\n","\n","# Example usage\n","download_images('SKZ', '/content/drive/MyDrive/fake-news-hands-on-20231225T095255Z-001/fake-news-hands-on/cats_images', num_images=10)\n","\n","df_part = df.iloc[60:70]\n","for index, row in df_part.iterrows():\n","    text_content = row['text']\n","    tweet_id = row['tweet_id']\n","    print(\"text_content:\",text_content)\n","    # Example: Download image\n","    image_save_path = f'/content/drive/MyDrive/fake-news-hands-on-20231225T095255Z-001/fake-news-hands-on/images/{tweet_id}_image.jpg'  # Adjust the path\n","    download_images(text_content, image_save_path)\n"]},{"cell_type":"markdown","metadata":{"id":"1-bC3gAWfRD2"},"source":["**USING TWEEPY**\n","\n","### FAQs for the Twitter API Code:\n","\n","**Q1: What is the purpose of the first few lines of code, including the imports and mounting Google Drive?**\n","*The code imports necessary libraries (`tweepy`, `requests`, `pandas`, and `OAuthHandler` from `tweepy`) and mounts Google Drive to access files stored in it. It also loads Twitter API credentials from a JSON file.*\n","\n","**Q2: How are Twitter API credentials loaded from the JSON file?**\n","*The code reads the Twitter API credentials from a JSON file (`twitter_credentials.json`) using the `json.load` method. The credentials are then stored in the `credentials` variable.*\n","\n","**Q3: How does the code set up Tweepy authentication using the loaded credentials?**\n","*The code uses the `tweepy.OAuthHandler` class to set up OAuth authentication with Twitter API by providing the consumer key, consumer secret, access token, and access secret from the loaded credentials.*\n","\n","**Q4: What does the `fetch_tweet_content` function do?**\n","*The `fetch_tweet_content` function takes a tweet ID as input and uses Tweepy to fetch the full text content of the tweet with the specified ID. If successful, it returns the text content; otherwise, it prints an error message.*\n","\n","**Q5: How can I modify the code to fetch additional tweet information?**\n","*To fetch additional information, you can explore the attributes of the `tweet` object returned by the `api.get_status` method. For example, you can access the timestamp, user information, or other metadata.*\n","\n","**Q6: What is the purpose of the `download_tweet_image` function?**\n","*The `download_tweet_image` function downloads the image associated with a tweet using its tweet ID. It retrieves the image URL from the tweet's media entities and saves the image to the specified path.*\n","\n","**Q7: How does the loop (`for index, row in df.head(10).iterrows():`) work?**\n","*The loop iterates over the first 10 rows of the DataFrame (`df`) using `df.head(10).iterrows()`. For each row, it fetches the tweet content and downloads the associated image using the `fetch_tweet_content` and `download_tweet_image` functions.*\n","\n","**Q8: What information does the line `print(f\"Tweet Content for tweet with ID {tweet_id}:\\n{text_content}\")` display?**\n","*This line prints the tweet ID and the full text content of the tweet. It is useful for inspecting the content of each tweet fetched in the loop.*\n","\n","**Q9: How is the image save path constructed in the loop (`image_save_path = f'/content/drive/MyDrive/fake-news-hands-on/images/{tweet_id}_image.jpg'`)?**\n","*Similar to the previous example, the image save path is constructed using the tweet ID to ensure a unique path for each image.*\n","\n","**Q10: What should I do if there are issues with fetching tweet content or downloading images?**\n","*Check the error messages printed during execution for clues. Ensure that the Twitter API credentials are correct, and the tweet IDs in the DataFrame are valid. Also, verify that the images are publicly accessible and not protected by privacy settings.*"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8420,"status":"ok","timestamp":1703498805674,"user":{"displayName":"Yasaswi B","userId":"00339005883420863081"},"user_tz":-330},"id":"87wicM8yfZXT","outputId":"b6772e59-e567-412e-fbd6-234479ff0abd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Error fetching tweet with ID 553477352542707712: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 553477352542707712:\n","None\n","Error downloading image for tweet with ID 553477352542707712: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Error fetching tweet with ID 553589285342175232: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 553589285342175232:\n","None\n","Error downloading image for tweet with ID 553589285342175232: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Error fetching tweet with ID 552810804186460161: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 552810804186460161:\n","None\n","Error downloading image for tweet with ID 552810804186460161: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Error fetching tweet with ID 553535971959275520: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 553535971959275520:\n","None\n","Error downloading image for tweet with ID 553535971959275520: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Error fetching tweet with ID 553579224402235393: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 553579224402235393:\n","None\n","Error downloading image for tweet with ID 553579224402235393: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Error fetching tweet with ID 552830864187727876: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 552830864187727876:\n","None\n","Error downloading image for tweet with ID 552830864187727876: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Error fetching tweet with ID 553103334626701312: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 553103334626701312:\n","None\n","Error downloading image for tweet with ID 553103334626701312: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Error fetching tweet with ID 553152215050125313: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 553152215050125313:\n","None\n","Error downloading image for tweet with ID 553152215050125313: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Error fetching tweet with ID 552796091255894016: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 552796091255894016:\n","None\n","Error downloading image for tweet with ID 552796091255894016: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Error fetching tweet with ID 553536269972942849: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n","Tweet Content for tweet with ID 553536269972942849:\n","None\n","Error downloading image for tweet with ID 553536269972942849: 403 Forbidden\n","453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n"]}],"source":["import tweepy\n","import requests\n","import pandas as pd\n","from tweepy import OAuthHandler, TweepyException\n","\n","from google.colab import drive\n","import json\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Path to the JSON file with Twitter API credentials\n","credentials_path = '/content/drive/MyDrive/fake-news-hands-on-20231225T095255Z-001/fake-news-hands-on/twitter_credentials.json'\n","\n","# Read the credentials from the file\n","with open(credentials_path) as f:\n","    credentials = json.load(f)\n","\n","# Set up Tweepy authentication\n","auth = tweepy.OAuthHandler(credentials['consumer_key'], credentials['consumer_secret'])\n","auth.set_access_token(credentials['access_token'], credentials['access_secret'])\n","api = tweepy.API(auth)\n","\n","\n","# Function to fetch tweet content by tweet ID\n","def fetch_tweet_content(tweet_id):\n","    try:\n","        tweet = api.get_status(tweet_id, tweet_mode='extended')\n","        text_content = tweet.full_text\n","        return text_content\n","    except tweepy.TweepyException as e:\n","        print(f\"Error fetching tweet with ID {tweet_id}: {e}\")\n","        return None\n","\n","# Function to download image from tweet by tweet ID\n","def download_tweet_image(tweet_id, save_path):\n","    try:\n","        tweet = api.get_status(tweet_id, tweet_mode='extended')\n","        media = tweet.entities.get('media', [])\n","        if media:\n","            image_url = media[0]['media_url_https']\n","            response = requests.get(image_url)\n","            with open(save_path, 'wb') as file:\n","                file.write(response.content)\n","            print(f\"Image downloaded for tweet with ID {tweet_id}\")\n","        else:\n","            print(f\"No image found for tweet with ID {tweet_id}\")\n","    except tweepy.TweepyException as e:\n","        print(f\"Error downloading image for tweet with ID {tweet_id}: {e}\")\n","\n","# Loop over tweet IDs in the DataFrame\n","for index, row in df.head(10).iterrows():\n","    tweet_id = row['tweet_id']\n","\n","    # Example: Fetch tweet content\n","    text_content = fetch_tweet_content(tweet_id)\n","    print(f\"Tweet Content for tweet with ID {tweet_id}:\\n{text_content}\")\n","\n","    # Example: Download image\n","    image_save_path = f'/content/drive/MyDrive/fake-news-hands-on-20231225T095255Z-001/fake-news-hands-on/images/{tweet_id}_image.jpg'  # Adjust the path\n","    download_tweet_image(tweet_id, image_save_path)\n"]},{"cell_type":"markdown","metadata":{"id":"ezbdhNgjBtOu"},"source":["**USING BERTWEET**\n","\n","### FAQs for the BERTweet Code:\n","\n","**Q1: What is the purpose of the first few lines of code, including the imports and loading the BERTweet model and tokenizer?**\n","*The code uses the `transformers` library to load the BERTweet model (`vinai/bertweet-base`) and tokenizer. The model is then set up to tokenize and extract features from a given input tweet.*\n","\n","**Q2: How is the BERTweet model initialized and loaded?**\n","*The BERTweet model is initialized using `AutoModel.from_pretrained(\"vinai/bertweet-base\")`, which loads the pre-trained BERTweet model. Similarly, the tokenizer is loaded using `AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)`.*\n","\n","**Q3: Why are there two different ways to initialize the tokenizer depending on the transformers version?**\n","*Transformers v4.x and above use the `AutoTokenizer` with `use_fast=False`, while v3.x uses the same `AutoTokenizer` without this parameter. This is due to changes in the transformers library versions.*\n","\n","**Q4: What does the code block with `input_ids` and `features` do?**\n","*This block tokenizes the input tweet, converts it into input IDs, and then uses the BERTweet model to obtain features (last hidden states) for each token in the input tweet. The shape of the last hidden states is printed.*\n","\n","**Q5: How does the code block with `features = bertweet(input_ids).last_hidden_state` differ from the previous block?**\n","*This block performs the same operations but extracts the last hidden state directly. The output is assigned to the `features` variable.*\n","\n","**Q6: What does `features.mean(dim=1)` do in the code?**\n","*This code calculates the mean of the features along the sequence dimension (axis 1). It is a common practice to pool the sequence of hidden states into a single vector, which is often used as a representation of the entire input sequence.*\n","\n","**Q7: Can I use the sum instead of the mean for pooling the features?**\n","*Yes, you can use `features.sum(dim=1)` to sum the features instead of taking the mean. The choice between mean and sum depends on the specific requirements of your task.*\n","\n","**Q8: How can I use BERTweet features for downstream tasks like classification or regression?**\n","*You can use the pooled features as input to a classifier or regression model. The output of the classifier can then be used for tasks such as sentiment analysis, text classification, or any other task the model was trained on.*\n","\n","**Q9: What should I do if I encounter memory issues when using BERTweet?**\n","*If you encounter memory issues, you may consider using a smaller batch size or use a machine with a larger GPU memory. Additionally, you can explore model quantization techniques or use a smaller variant of the BERTweet model if available.*\n","\n","**Q10: Can I fine-tune the BERTweet model on my own dataset?**\n","*BERTweet is a pre-trained model, and fine-tuning it requires additional steps and resources. You can refer to the Hugging Face documentation on fine-tuning transformers models for more details.*"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8488,"status":"ok","timestamp":1703498830911,"user":{"displayName":"Yasaswi B","userId":"00339005883420863081"},"user_tz":-330},"id":"SQ5E6iroBkio","outputId":"1eda2f41-f8cf-4d85-bcdb-ad0c58c01687"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9156,"status":"ok","timestamp":1703486149159,"user":{"displayName":"raghvendra kumar","userId":"16530528870044820867"},"user_tz":-330},"id":"xeBn53KsCkEc","outputId":"5fd913c8-0389-4717-cae5-ba94213296c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: emoji==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n"]}],"source":["!pip3 install emoji==0.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72Nns9oLB_TR","executionInfo":{"status":"ok","timestamp":1703486167140,"user_tz":-330,"elapsed":17985,"user":{"displayName":"raghvendra kumar","userId":"16530528870044820867"}},"outputId":"c4583af8-3cce-493b-8c99-987660f2a797"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 21, 768])\n","torch.Size([1, 768])\n"]}],"source":["import torch\n","from transformers import AutoModel, AutoTokenizer\n","\n","bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n","\n","# For transformers v4.x+:\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n","\n","# For transformers v3.x:\n","# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n","\n","# INPUT TWEET IS ALREADY NORMALIZED!\n","line = \"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:\"\n","\n","input_ids = torch.tensor([tokenizer.encode(line)])\n","\n","with torch.no_grad():\n","    features = bertweet(input_ids)  # Models outputs are now tuples\n","\n","print(features.last_hidden_state.shape)\n","\n","# With TensorFlow 2.0+:\n","# from transformers import TFAutoModel\n","# bertweet = TFAutoModel.from_pretrained(\"vinai/bertweet-base\")\n","\n","with torch.no_grad():\n","    features = bertweet(input_ids).last_hidden_state  # Extract the last hidden state\n","\n","# Take the mean or sum along the sequence dimension (axis 1)\n","pooled_features = features.mean(dim=1)  # You can use .sum(dim=1) for summing instead of averaging\n","\n","# Print the shape of the pooled features\n","print(pooled_features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMivQuLUYwqj","executionInfo":{"status":"ok","timestamp":1703486167141,"user_tz":-330,"elapsed":16,"user":{"displayName":"raghvendra kumar","userId":"16530528870044820867"}},"outputId":"e4c8c970-3aac-4caf-9255-69076f64ee3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-1.9185e-01,  4.4780e-03, -4.1096e-02,  5.2936e-02, -6.6554e-03,\n","          1.7550e-01,  1.8320e-01,  7.6464e-02,  3.9108e-01, -1.0206e-01,\n","         -3.1259e-02,  3.4986e-01, -3.4544e-02,  1.3942e-01,  1.1159e-01,\n","          6.6492e-02,  6.1305e-02,  1.4109e-01,  7.9099e-02,  5.4069e-02,\n","         -2.0409e-01,  1.0717e-01,  1.6638e-01, -3.8153e-02,  2.1669e-01,\n","         -6.3921e-02,  6.3063e-03,  9.7963e-02,  3.0139e-01, -6.1971e-02,\n","          1.3789e-01, -4.5724e-03,  5.3157e-02, -2.1940e-02,  1.0253e-01,\n","         -1.0205e-01,  9.2922e-02,  1.2522e-01,  5.1839e-02, -2.6070e-01,\n","          1.0986e-01, -1.8352e-01,  2.8338e-02,  1.0956e-01,  4.4443e-01,\n","         -4.9507e-02,  1.7158e-02, -3.9355e-02,  8.5930e-02, -1.0141e-02,\n","         -1.3424e-01, -1.1491e-01,  3.8322e-01, -5.7799e-02,  1.8786e-01,\n","         -6.8989e-02,  1.1009e-01,  1.5372e-01, -9.5463e-02,  5.3597e-02,\n","         -1.8781e-02,  7.8860e-04,  2.8826e-01,  8.0866e-02,  3.0732e-01,\n","         -6.3989e-02,  1.0702e-01, -1.7252e-02,  2.1184e-01, -2.0677e-02,\n","          7.0096e-02,  9.7305e-02,  1.2890e-01,  9.0726e-02, -1.7694e-01,\n","          3.6728e-02, -7.2761e-03, -7.3430e-02,  3.6645e-01,  5.2082e-02,\n","         -1.2405e-01,  1.1991e-01, -6.9234e-02, -2.2209e-01,  2.8737e-01,\n","          3.6354e-02,  6.0132e-02, -2.9305e-01,  6.0758e-03,  7.0007e-02,\n","          4.8135e-02,  2.9367e-01,  1.4010e-01,  3.1431e-02,  9.4493e-02,\n","          5.6756e-02,  6.9404e-02, -2.4500e-01,  2.9043e-02,  1.1727e-02,\n","         -1.3881e-01,  7.1394e-02,  1.1559e-01,  2.7504e-02, -6.3860e-02,\n","         -1.0678e-01, -3.5299e-02, -1.2552e-01,  1.4942e-02, -4.1305e-02,\n","          1.4080e-01, -4.5313e-02, -4.7791e-02,  1.0244e-01, -5.7552e-02,\n","         -1.5966e-01, -2.2275e-02,  9.1299e-02, -1.5770e-05, -4.6417e-02,\n","         -1.0725e-01,  2.1129e-01,  6.9765e-01,  1.8634e-01, -7.8421e-02,\n","          2.3500e-01, -1.3471e-01, -4.0409e-02, -3.8161e-02, -1.9080e-01,\n","         -6.7982e-02,  7.9765e-02, -1.8655e-02,  8.4809e-02,  2.2784e-01,\n","          2.0973e-01, -2.7167e-02,  8.5814e-03, -1.1331e-01, -1.9716e-01,\n","         -9.6052e-02,  3.2089e-01,  1.3040e-01,  1.0142e-01, -6.8030e-02,\n","          1.3007e-01, -1.5996e-02,  1.4283e-01,  2.5365e-01,  3.2782e-02,\n","          1.9663e-01,  1.7457e-01,  5.7409e-03,  8.1551e-02, -2.2767e-01,\n","          8.3403e-02,  1.4090e-01, -1.1530e-01,  1.1196e-01, -1.1762e-01,\n","          5.8113e-01,  2.1348e-03, -2.2860e-02,  4.7532e-02, -5.0286e-02,\n","          1.1227e-01,  1.4127e-01,  1.3931e-01,  1.6651e-01, -3.8409e-01,\n","          2.1337e-01,  2.9665e-01,  7.8395e-02, -6.5740e-02,  5.8288e-02,\n","          8.4936e-02,  2.5670e-02,  9.5296e-02,  1.0301e-01,  2.5238e-01,\n","          1.1586e-01, -5.7704e-02,  8.8925e-03, -8.0727e-02,  2.6732e-01,\n","          1.7474e-01,  7.8024e-01,  1.9515e-01,  1.4401e-01,  3.5669e-01,\n","         -1.2959e-01,  1.3770e-01,  2.1782e-01,  2.6041e-02,  1.7286e-01,\n","          3.3863e-01, -5.8708e-01,  2.4868e-02, -9.2692e-02,  9.2447e-02,\n","          1.3206e-01,  5.2666e-02,  3.3289e-03, -2.4764e-01,  4.3860e-02,\n","          2.1629e-01,  1.4157e-01, -5.3051e-02, -2.5780e-02, -6.0569e-02,\n","          5.4810e-02, -5.5210e-02, -1.4694e-01,  3.8962e-02,  2.9055e-01,\n","          1.4033e+00, -4.1111e-03, -2.3633e-01,  1.8516e-01,  1.5666e-01,\n","          3.4404e-03,  4.5884e-02,  2.0579e-01, -1.0913e-01,  6.4857e-02,\n","         -8.2531e-02,  3.7121e-02,  4.9761e-02, -1.0121e-01, -2.5219e-02,\n","          8.6246e-02, -2.5711e-01,  1.7463e-02,  1.2136e-02,  2.6839e-02,\n","          1.7904e-01,  1.4230e-01, -3.4526e-02,  2.2004e-01,  7.6416e-02,\n","         -1.9328e-01, -8.9167e-02, -7.8175e-02,  2.3367e-02,  1.8999e-02,\n","         -7.2326e-03,  3.8850e-02, -1.2099e-02, -1.1453e-01,  3.6644e-01,\n","          1.0375e-01, -7.6986e-02,  2.5852e-01, -6.2412e-02, -3.3467e-02,\n","          9.5207e-02,  1.7109e-01,  7.0085e-02,  4.7674e-03,  1.4200e-01,\n","          1.0448e-01, -9.8430e-02,  2.5007e-01, -4.0169e-03, -6.3931e-03,\n","         -1.0018e-01,  1.7947e-01, -8.5979e-04,  5.0323e-02,  9.4918e-02,\n","          1.8855e-01, -4.7386e-02,  9.2385e-02, -5.5756e-02,  1.0851e-01,\n","          1.7935e-02, -5.5640e-02,  1.8340e-01,  5.5878e-02, -4.2401e-03,\n","          5.2971e-01,  2.2708e-01, -1.5733e-01,  6.3350e-02, -5.0744e-02,\n","          9.8593e-02, -1.3829e-01, -9.1569e-02,  1.0707e-02,  1.1696e-01,\n","         -1.0127e-01, -1.6531e-01,  4.8542e-02,  2.6339e-01,  5.7554e-02,\n","          1.0910e-02, -1.9047e-02, -2.1681e-01,  3.8951e-02, -6.1807e-03,\n","         -8.8926e-02, -8.6057e-02, -2.0478e-01,  1.2395e-02,  3.2715e-02,\n","         -1.9342e-02, -1.2662e-02, -1.8274e-02,  9.4349e-02, -2.0249e-01,\n","          1.4715e-01,  4.7503e-02,  1.1675e-01, -2.3610e-01,  3.6717e-01,\n","          1.1981e-01, -1.8179e-01,  1.5873e-01, -3.7830e-02, -8.3333e-02,\n","          6.8860e-02,  7.6493e-03,  1.8883e-01,  8.9084e-02, -6.8701e-02,\n","         -7.4066e-03,  9.9883e-02,  1.4253e-01,  2.6744e-02, -7.0173e-02,\n","          3.0172e-01, -1.2722e-01, -1.4396e-01,  1.9041e-01,  6.7874e-02,\n","         -3.3282e-02,  7.5368e-02,  1.0446e-02, -1.8468e-01,  1.2240e-01,\n","          3.5316e-01,  5.8655e-02, -3.0838e-02, -6.0137e-01,  2.2097e-01,\n","          2.6422e-01, -3.2896e-01,  2.6003e-01,  1.5228e-01,  1.2619e-01,\n","          7.9990e-02, -1.5480e-01,  2.3553e-01,  2.0723e-02,  1.0131e-01,\n","          2.3029e-01,  1.4063e-01, -2.1030e-02, -9.5463e-02, -1.1952e-01,\n","         -3.0925e-02, -5.9430e-02,  2.0905e-02,  6.9471e-02,  2.7989e-02,\n","         -2.1993e-02,  2.4746e-01,  4.9116e-03, -3.6297e-02,  5.0270e-02,\n","         -1.8208e-01,  2.0151e-01,  1.7265e-01,  4.9241e-02, -4.8625e-02,\n","         -4.4284e-01, -9.7470e-02,  1.4110e+00, -2.6540e-02,  8.6250e-02,\n","          1.4599e-01,  2.7340e-01, -6.4444e-02, -4.5666e-03,  1.7096e-02,\n","          3.5969e-02, -3.4462e-02,  6.4436e-02, -7.7873e-02,  4.1759e-02,\n","         -1.0749e-01, -2.7919e-02,  1.1602e-02, -2.3266e-01,  4.5856e-02,\n","          1.0524e-01,  2.2571e-01,  8.0552e-02, -4.9514e-02, -5.1286e-03,\n","          2.4845e-01,  2.0898e-02,  2.3999e-01, -4.2010e-02,  1.4557e-01,\n","          4.3924e-02, -5.8819e-02,  4.1444e-01,  2.0889e-01,  1.4994e-01,\n","          1.3647e-01, -1.0897e-01,  3.0610e-01,  5.2546e-02, -4.1947e-01,\n","         -4.1839e-02,  2.0642e-01,  9.3785e-02,  1.1253e-03,  7.2969e-02,\n","          4.4780e-01,  3.9666e-02, -2.9624e-01, -5.8278e-02, -5.2596e-02,\n","          1.2582e-04,  2.1570e-01,  1.9370e-01, -2.6650e-01, -6.1905e-02,\n","          1.4618e-01,  4.8117e-02, -3.2052e-02,  1.0137e-01,  1.4360e-01,\n","         -1.0774e-01,  6.7888e-02,  1.9475e-01,  2.7393e-02,  5.7627e-02,\n","         -1.3998e-01,  7.1735e-02,  1.1159e-01, -9.3218e-02, -1.6249e-01,\n","          1.6102e+00,  4.2434e-02,  8.6042e-02, -6.9209e-02,  3.7680e-01,\n","          1.0585e-01,  2.3075e-01, -1.7858e-01,  1.1642e-01, -1.3946e-01,\n","          1.2544e-01,  5.6961e-02, -1.6026e-01,  2.9798e-02, -1.9035e-01,\n","          2.6819e-02,  1.4313e-01,  1.7164e-01,  2.4715e-01,  2.0759e-01,\n","          4.7751e-03,  6.1371e-02, -2.1417e-01,  1.3616e-01, -5.5457e-02,\n","         -1.9596e-01,  4.1164e-02,  7.4527e-02,  1.3612e-01, -9.1944e-03,\n","          7.4132e-03, -1.7606e-01, -1.0625e-01, -2.4318e-01, -2.5031e-02,\n","          1.3307e-01, -1.3899e-01, -9.3094e-02,  2.6295e-01,  1.3751e-01,\n","         -4.3554e-02,  1.5957e-01,  5.6080e-01, -8.8671e-02, -1.3226e-01,\n","          1.0444e-01, -5.5460e-03,  5.0419e-01, -1.7098e-03, -2.8644e-02,\n","         -2.3676e-01,  6.9951e-02, -5.0991e-03,  1.9996e-02,  7.2984e-03,\n","          2.3284e-02,  1.3219e-01, -1.9990e-01,  4.7961e-02,  2.5309e-01,\n","          1.5264e-01, -1.0722e-01,  2.2574e-01, -4.0778e-01, -4.2510e-02,\n","         -1.2228e-01, -1.1143e-01, -5.7115e-02, -1.9481e-03,  2.7909e-01,\n","         -1.3298e-02,  1.6101e-01, -4.1950e-02,  2.7918e-01,  1.5266e-02,\n","          2.7079e-02,  5.9377e-02, -6.0417e-03, -5.0891e-02, -2.2700e-02,\n","          3.0696e-01, -1.3837e-02,  1.2466e-01,  9.6462e-02,  1.1275e-01,\n","          4.1910e-02,  7.8273e-02,  2.7656e-01,  4.5640e-01,  1.4541e-01,\n","          9.8001e-02,  2.0392e-01,  3.7938e-02,  2.5457e-01,  3.0151e-01,\n","          1.2652e-01, -1.2289e-02, -8.7494e-02,  3.0350e-01, -5.6476e-02,\n","          2.3915e-01,  6.7942e-02,  1.3125e-01, -3.0855e-02,  1.0750e-01,\n","         -5.4387e-01, -9.8924e-03, -4.4055e-02, -5.1373e-02, -3.4532e-02,\n","          4.0397e-02,  2.5599e-02,  6.5991e-02,  1.9340e-01,  3.1543e-01,\n","          6.1846e-02,  2.1474e-02,  1.0912e-01,  3.7675e-02, -9.5326e-02,\n","         -2.6195e-01, -2.2164e-01,  2.2998e-01,  3.5745e-02,  1.7591e-01,\n","         -3.1020e-01,  1.2640e-01,  1.6114e-01,  2.4182e-02,  1.7582e-01,\n","          1.1138e-01, -7.9425e-02,  1.9184e-01,  4.6643e-02, -2.5713e-01,\n","          4.1024e-01, -1.6504e-01,  1.6640e-01,  2.2633e-02,  5.0186e-02,\n","         -1.3601e-01,  1.0854e-02, -2.4997e-01,  1.1565e-01,  2.0172e-01,\n","          2.6763e-02,  2.4907e-01, -4.8799e-02,  1.1639e-01, -3.8085e-02,\n","          8.1627e-02, -5.7794e-02,  1.9059e-01, -3.2665e-02,  3.3520e-02,\n","         -4.8813e-02, -1.4173e-01,  1.6653e-01,  2.7775e-01,  3.8425e-02,\n","         -9.1829e-02,  3.7261e-02, -1.2446e-01, -1.6011e-01,  2.7864e-01,\n","         -1.8736e-02, -3.1492e-02, -7.4671e-02,  2.1836e-02,  3.8339e-01,\n","          3.7995e-02,  1.6743e-01, -1.5186e-01,  1.2021e-01, -2.8637e-02,\n","          8.3043e-02, -2.4820e-01,  8.5570e-02,  3.0441e-03,  1.6108e-01,\n","          5.5406e-03, -1.0868e-01,  1.0888e-01,  1.0553e-01,  7.0598e-02,\n","          1.7966e-01,  9.5725e-02,  2.3495e-02, -5.9115e-02,  2.0095e-01,\n","         -1.3096e-01, -7.4417e-02,  1.1284e-01, -1.5319e-01,  1.6796e-01,\n","          2.5031e-01,  2.5548e-02,  2.0630e-01,  9.2204e-02,  6.7497e-02,\n","         -6.7665e-02,  2.1012e-02,  8.1127e-02, -4.4566e-01,  5.4628e-03,\n","          3.0082e-01,  1.8701e-01,  7.0616e-02, -1.8995e-01, -1.6555e-01,\n","          1.8165e-01,  4.7425e-03, -1.5523e-01,  1.4366e-02,  1.2917e-01,\n","          2.1765e-01, -8.0853e-02,  3.4938e-01,  2.7627e-01, -1.6884e-01,\n","          9.2528e-02,  1.3660e-01, -1.2491e-01, -2.0967e-02, -1.7692e-01,\n","         -8.5720e-01,  2.5881e-01,  4.5883e-01, -5.1960e-02, -4.3885e-02,\n","          1.6550e-01,  1.2106e-01, -1.0945e-01,  1.0948e-01,  1.0394e-01,\n","          2.2852e-01,  2.6209e-01,  6.8987e-02, -7.9363e-02,  1.4048e-01,\n","          2.1363e-01, -4.6369e-03,  1.3130e-01,  6.6308e-02,  1.5279e-01,\n","          1.8087e-01, -1.1371e-02,  4.8269e-02, -3.5812e-01,  1.2323e-01,\n","         -9.1762e-02, -1.7460e-02,  2.9276e-02, -9.8531e-02,  1.5139e-02,\n","         -5.4776e-02,  6.3301e-02,  1.2366e-01,  9.3760e-02,  3.6376e-03,\n","         -7.4175e-02, -1.9689e-01,  6.1362e-02, -9.5919e-02,  3.0864e-01,\n","         -1.1870e-01,  1.4009e-01,  6.7884e-02,  1.5732e-01,  3.4483e-02,\n","         -4.5595e-02,  1.3006e-01,  1.0065e-01, -6.3772e-02, -1.0370e-01,\n","          1.8309e-01,  1.7684e-02,  1.4847e-01, -6.1531e-02, -4.9930e-02,\n","          1.3391e-01,  5.8490e-02,  1.7299e-01, -9.0433e-02, -3.6813e-02,\n","         -2.0523e-03, -5.0512e-02, -1.7035e-01, -4.6227e-02, -3.3727e-02,\n","         -6.5367e-02,  1.0549e-01,  6.9036e-02, -7.3201e-02, -2.7930e-02,\n","         -8.9750e-02,  1.3140e-01,  1.5556e-01,  1.6493e-01, -1.3353e-01,\n","          1.7886e-01,  6.2894e-03,  2.7723e-02,  1.3267e-01,  1.2623e-03,\n","         -1.6535e-01,  1.0930e-01,  1.2288e-01, -1.4000e-01,  8.0334e-03,\n","          2.3651e-01,  3.1297e-01,  2.1200e-01,  9.0410e-02, -8.0606e-02,\n","          2.1529e-02, -1.4431e-01,  1.3392e-01, -3.0679e-02,  6.1258e-02,\n","          2.6392e-02,  1.4683e-01, -6.8740e-02]])\n"]}],"source":["print(pooled_features)"]},{"cell_type":"markdown","metadata":{"id":"yPw0JNKdaCV8"},"source":["**Transformers Overview:**\n","\n","**1. Introduction:**\n","   - **Transformer Architecture:** Transformers represent a breakthrough in the field of natural language processing (NLP) and have been extended to various domains. The architecture was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017.\n","\n","**2. Key Features:**\n","   - **Attention Mechanism:** The core innovation in transformers is the attention mechanism, which allows the model to focus on different parts of the input sequence when making predictions. It enables capturing long-range dependencies efficiently.\n","   - **Parallelization:** Transformers are highly parallelizable, making them suitable for training on hardware like GPUs and TPUs. This parallelization is a result of the attention mechanism's ability to process input sequences in parallel.\n","\n","**3. Architecture Details:**\n","   - **Encoder-Decoder Structure:** Transformers are commonly structured into an encoder and a decoder. The encoder processes the input sequence, and the decoder generates the output sequence. In tasks like language translation, the encoder processes the source language, and the decoder generates the target language.\n","   - **Multi-Head Attention:** Attention is applied across multiple heads, allowing the model to focus on different aspects of the input. This multi-head attention mechanism enhances the model's ability to capture various relationships in the data.\n","\n","**4. Pre-training and Transfer Learning:**\n","   - **Transfer Learning:** Transformers are often pre-trained on large corpora using unsupervised learning objectives. Pre-training allows the model to learn rich representations of the input data.\n","   - **Fine-Tuning:** Pre-trained transformers can be fine-tuned on downstream tasks with smaller labeled datasets. This transfer learning approach has proven highly effective across various NLP tasks.\n","\n","**5. Applications:**\n","   - **Natural Language Processing:** Transformers have achieved state-of-the-art performance in a wide range of NLP tasks, including machine translation, sentiment analysis, text summarization, and named entity recognition.\n","   - **Computer Vision:** Transformers have been extended to computer vision tasks, such as image classification, object detection, and image generation. Vision Transformers (ViTs) have shown success in replacing convolutional neural networks (CNNs) in certain tasks.\n","\n","**6. Architectural Variants:**\n","   - **BERT (Bidirectional Encoder Representations from Transformers):** BERT is a pre-trained transformer designed for bidirectional language understanding. It has been foundational in various NLP tasks.\n","   - **GPT (Generative Pre-trained Transformer):** GPT models, developed by OpenAI, are autoregressive transformers used for generating coherent and context-aware text.\n","   - **T5 (Text-to-Text Transfer Transformer):** T5 is a transformer model designed to handle various NLP tasks in a unified text-to-text framework.\n","\n","**7. Library Support:**\n","   - **Hugging Face Transformers:** The Hugging Face Transformers library is a widely-used open-source library that provides pre-trained transformer models, including BERT, GPT, and others. It facilitates easy integration and fine-tuning of transformer models.\n","\n","**8. Limitations and Considerations:**\n","   - **Computational Intensity:** Training large transformer models can be computationally intensive and may require specialized hardware.\n","   - **Interpretability:** Understanding the inner workings of transformers, especially with a large number of parameters, remains a challenge.\n","\n","**9. Further Reading:**\n","   - **Original Paper:** \"Attention is All You Need\" by Ashish Vaswani et al. (NeurIPS, 2017) [Link](https://arxiv.org/abs/1706.03762)\n","\n","Transformers have significantly impacted the field of deep learning and are widely adopted in both academia and industry for their effectiveness in capturing complex patterns and relationships in sequential data."]}],"metadata":{"colab":{"provenance":[{"file_id":"1rNlcvr7iT98UB0a0SPdaVQMBSKHQVF06","timestamp":1703497050553}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}